{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/spambase.data\", dtype='f', delimiter=',')\n",
    "\n",
    "print (data)\n",
    "print (data.dtype)\n",
    "\n",
    "max_value_in_data = np.zeros(57)\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 57):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que teremos que tratar os dados antes de rodar __Online SVM via Online Gradient Descent__\n",
    "\n",
    "Temos ao menos duas etapas:\n",
    "1 - Transformar os valores de classe ( variável target ) de 0, 1 para -1 ,1\n",
    "2 - Temos que fazer uma normalização dos parâmetros. Pois alguns dados são percentuais e estão no range [0, 100] e outros podem atingir valores maiores como por exemplo 15841. Não queremos que um atributo tenha uma importância maior que outro por ser representado em uma escala diferente. Vamos fazer uma normalização __hard__ que é mapear o maior valor de uma determinada feature para 1 e dividir os outros valores dessa feature específica pelo maior valor encontrado.\n",
    "\n",
    "Se a normalização hard se mostrar ruim na prática, podemos pensar em outras normalizações. ( Soft usando mediana e desvio-padrão )..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# vamos alterar os valores da variável target de (0, 1) para (-1, 1)\n",
    "for lin in data:\n",
    "    if lin[57] == 0:\n",
    "        lin[57] = -1\n",
    "        \n",
    "# Agora vamos fazer a normalizacao hard\n",
    "normalized_data = data.copy()\n",
    "\n",
    "print( normalized_data.dtype )\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 57):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        \n",
    "for lin in normalized_data:\n",
    "    for x in range(0, 57):\n",
    "        lin[x] = (lin[x] / max_value_in_data[x] )\n",
    "\n",
    "        \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 4.48179282e-02, 1.25490189e-01, ...,\n",
       "        6.00720895e-03, 1.74873751e-02, 1.00000000e+00],\n",
       "       [4.62555066e-02, 1.96078438e-02, 9.80392173e-02, ...,\n",
       "        1.00120148e-02, 6.48358613e-02, 1.00000000e+00],\n",
       "       [1.32158585e-02, 0.00000000e+00, 1.39215678e-01, ...,\n",
       "        4.84581478e-02, 1.42550513e-01, 1.00000000e+00],\n",
       "       ...,\n",
       "       [6.60792962e-02, 0.00000000e+00, 5.88235334e-02, ...,\n",
       "        5.00600727e-04, 7.38636404e-03, 0.00000000e+00],\n",
       "       [2.11453736e-01, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        4.00480581e-04, 4.86111175e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.27450973e-01, ...,\n",
       "        4.00480581e-04, 2.46212143e-03, 0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  4.48179282e-02,  1.25490189e-01, ...,\n",
       "         6.10671751e-03,  1.75493974e-02,  1.00000000e+00],\n",
       "       [ 4.62555066e-02,  1.96078438e-02,  9.80392173e-02, ...,\n",
       "         1.01111224e-02,  6.48948923e-02,  1.00000000e+00],\n",
       "       [ 1.32158585e-02,  0.00000000e+00,  1.39215678e-01, ...,\n",
       "         4.85534072e-02,  1.42604634e-01,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 6.60792962e-02,  0.00000000e+00,  5.88235334e-02, ...,\n",
       "         6.00660744e-04,  7.44902482e-03, -1.00000000e+00],\n",
       "       [ 2.11453736e-01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         5.00550610e-04,  4.92393179e-03, -1.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  1.27450973e-01, ...,\n",
       "         5.00550610e-04,  2.52509303e-03, -1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descrição de Online SVM via Online Gradient Descent\n",
    "* No instante t\n",
    "    * Selecionamos um vetor pt\n",
    "    * Recebemos uma nova instância do dataset (yt, zt), onde yt são os atributos e zt é a classe correspondente\n",
    "    * Vamos tomar uma Hinge Loss definida como max(0, 1 - zt * <pt, yt> )\n",
    "    * Utilizaremos o gradiente da função de perda do instante t para calcular o próximo p(t+1)\n",
    "    \n",
    "Vamos também utilizar sempre pt pertencente ao espaço euclideano de norma <= 1.\n",
    "\n",
    "A princípio o p0 pode ser definido arbitrariamente, mas como vimos descrito em alguns lugares da literatura a inicialização com o vetor nulo vamos adotar essa estratégia.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui algumas funções auxiliares úteis\n",
    "def hinge_loss( p_t, z_t, y_t):\n",
    "    return max(0, 1 - np.dot(p_t, y_t) * z_t )\n",
    "\n",
    "def project_into_euclidean_norm_1( p_t ):\n",
    "    norm = np.linalg.norm(p_t)\n",
    "    norm_p = p_t.copy()\n",
    "    if norm <= 1:\n",
    "        return norm_p\n",
    "    else:\n",
    "        norm_p = np.true_divide( norm_p, norm )\n",
    "        return norm_p\n",
    "\n",
    "def get_gradient( y_t, z_t ):\n",
    "    gradient = y_t.copy()\n",
    "    gradient = gradient * (-z_t)\n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Vamos armazenar os pt obtidos ao longo do tempo, para poder analisar quão rápido o método alcança bons classificadores\\n# E no final vamos testar os valores obtidos como classificadores para o dataset completo e analisar a acurácia\\n all_p = []\\n\\ncurrent_p = [0] * 57\\n\\nall_p.append( current_p )\\n\\n# Temos algumas estratégias para definir o eta utilizado para atualizar p_t\\n# Vamos usar o eta proposto por Zinkevich que é da ordem de 1/sqrt(t)\\n# Que vai diminuindo conforme a quantidade de instancias processadas aumenta\\n\\n# Se tivéssemos a segunda derivada da função de perda poderiamos usar \\n# uma outra estratégia descoberta por Hazan em que eta_t = 1 / Ht, onde H é \\n# um valor relacionado com a segunda derivada\\n\\n# Essas estratégias para eta são definidas aqui (http://www.mit.edu/~rakhlin/papers/adaptive.pdf)\\n\\n\\ncumulative_loss = 0\\n# Agora vamos processar o dataset uma instancia de cada vez, num setup online\\n\\nt = 1\\n\\nfor instance in normalized_data:\\n    instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\\n    target_class = instance[57]\\n    # vamos tomar a perda hinge relativo a current_p\\n    current_loss = hinge_loss( current_p, target_class, instance_features )\\n    cumulative_loss += current_loss\\n    \\n    if t % 100 == 0:\\n        print('perda na iteracao %d = %f' % (t, current_loss) )\\n    \\n    # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\\n    # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\\n    \\n    # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\\n    if current_loss != 0:\\n        gradient = get_gradient( instance_features, target_class )\\n        current_eta = 1 / sqrt(t)\\n        # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\\n        for coord in range(57):\\n            current_p[coord] = (current_p[coord] - current_eta * gradient[coord])\\n        \\n        current_p = project_into_euclidean_norm_1( current_p )\\n            \\n        \\n    all_p.append( current_p ) # Adding current support vector to the list\\n    t += 1 # increment in one the number of processed instances\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Vamos armazenar os pt obtidos ao longo do tempo, para poder analisar quão rápido o método alcança bons classificadores\n",
    "# E no final vamos testar os valores obtidos como classificadores para o dataset completo e analisar a acurácia\n",
    " all_p = []\n",
    "\n",
    "current_p = [0] * 57\n",
    "\n",
    "all_p.append( current_p )\n",
    "\n",
    "# Temos algumas estratégias para definir o eta utilizado para atualizar p_t\n",
    "# Vamos usar o eta proposto por Zinkevich que é da ordem de 1/sqrt(t)\n",
    "# Que vai diminuindo conforme a quantidade de instancias processadas aumenta\n",
    "\n",
    "# Se tivéssemos a segunda derivada da função de perda poderiamos usar \n",
    "# uma outra estratégia descoberta por Hazan em que eta_t = 1 / Ht, onde H é \n",
    "# um valor relacionado com a segunda derivada\n",
    "\n",
    "# Essas estratégias para eta são definidas aqui (http://www.mit.edu/~rakhlin/papers/adaptive.pdf)\n",
    "\n",
    "\n",
    "cumulative_loss = 0\n",
    "# Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "\n",
    "t = 1\n",
    "\n",
    "for instance in normalized_data:\n",
    "    instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "    target_class = instance[57]\n",
    "    # vamos tomar a perda hinge relativo a current_p\n",
    "    current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "    cumulative_loss += current_loss\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print('perda na iteracao %d = %f' % (t, current_loss) )\n",
    "    \n",
    "    # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "    # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "    \n",
    "    # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "    if current_loss != 0:\n",
    "        gradient = get_gradient( instance_features, target_class )\n",
    "        current_eta = 1 / sqrt(t)\n",
    "        # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "        for coord in range(57):\n",
    "            current_p[coord] = (current_p[coord] - current_eta * gradient[coord])\n",
    "        \n",
    "        current_p = project_into_euclidean_norm_1( current_p )\n",
    "            \n",
    "        \n",
    "    all_p.append( current_p ) # Adding current support vector to the list\n",
    "    t += 1 # increment in one the number of processed instances\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora __encapsular__ o que foi implementado acima em um método, para poder testar diversos parâmetros de __eta__. E também vamos fazer uma função que mede a acurácia de um determinado vetor __p__ para classificar os dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_svm(data, eta_fnc):\n",
    "    current_p = np.zeros(57)\n",
    "    all_p = [ current_p ]\n",
    "    cumulative_loss = 0\n",
    "    # Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "    t = 1\n",
    "\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "        target_class = instance[57]\n",
    "        # vamos tomar a perda hinge relativo a current_p\n",
    "        current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "        cumulative_loss += current_loss\n",
    "        # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "        # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "\n",
    "        # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "        if current_loss > 0:\n",
    "            gradient = get_gradient( instance_features, target_class )\n",
    "            current_eta = eta_fnc( t ) # Eta calculado em funcao de t, de acordo com a funcao recebida como parametro\n",
    "            \n",
    "            # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "            cpy = current_p.copy()\n",
    "            \n",
    "            for coord in range(57):\n",
    "                cpy[coord] = (current_p[coord] - current_eta * gradient[coord])\n",
    "\n",
    "            cpy = project_into_euclidean_norm_1( cpy )\n",
    "            current_p = cpy.copy()\n",
    "            \n",
    "        tmp = np.append(all_p, [cpy], axis = 0 ) # Adding current support vector to the list\n",
    "        all_p = tmp.copy()\n",
    "        \n",
    "        t += 1 # increment in one the number of processed instances\n",
    "    return all_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que encapsulamos o método em uma função que recebe como parâmetro uma função para o cálculo de eta, podemos começar a realizar alguns experimentos.\n",
    "\n",
    "Antes disso, vou criar uma função que recebe um determinado vetor p, usa ele para __classificar__ todo o dataset e retorna a acurácia desse vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primeiro vetor =  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.39404477287546186 0.6059552271245382\n",
      "primeiro vetor =  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.39404477287546186 0.780699847859161\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(data, classifier_vector):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57].copy()\n",
    "        target_class = instance[57].copy()\n",
    "        dot_prod = np.dot( instance_features, classifier_vector )\n",
    "        predicted_class = -1\n",
    "        if dot_prod >= 0:\n",
    "            predicted_class = 1\n",
    "        if target_class == predicted_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return ( correct / total )\n",
    "\n",
    "\n",
    "def eta( t ):\n",
    "    return 1 / sqrt(t)\n",
    "\n",
    "def eta_constante( t ):\n",
    "    return 0.05\n",
    "\n",
    "test_data = normalized_data.copy()\n",
    "\n",
    "results2 = online_svm( test_data, eta )\n",
    "start_acc = check_accuracy( test_data, results2[0])\n",
    "end_acc = check_accuracy( test_data, results2[-1])\n",
    "print( start_acc, end_acc )\n",
    "\n",
    "np.random.seed(13)\n",
    "np.random.shuffle( test_data )\n",
    "\n",
    "\n",
    "results = online_svm( test_data, eta )\n",
    "start_acc2 = check_accuracy( test_data, results[0])\n",
    "end_acc2 = check_accuracy( test_data, results[-1])\n",
    "print( start_acc2, end_acc2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
