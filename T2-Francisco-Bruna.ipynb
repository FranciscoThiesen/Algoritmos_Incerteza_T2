{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/spambase.data\", dtype='f', delimiter=',')\n",
    "\n",
    "print (data)\n",
    "print (data.dtype)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que teremos que tratar os dados antes de rodar __Online SVM via Online Gradient Descent__\n",
    "\n",
    "Temos ao menos duas etapas:\n",
    "1 - Transformar os valores de classe ( variável target ) de 0, 1 para -1 ,1\n",
    "2 - Temos que fazer uma normalização dos parâmetros. Pois alguns dados são percentuais e estão no range [0, 100] e outros podem atingir valores maiores como por exemplo 15841. Não queremos que um atributo tenha uma importância maior que outro por ser representado em uma escala diferente. Vamos fazer uma normalização __hard__ que é mapear o maior valor de uma determinada feature para 1 e dividir os outros valores dessa feature específica pelo maior valor encontrado.\n",
    "\n",
    "Se a normalização hard se mostrar ruim na prática, podemos pensar em outras normalizações. ( Soft usando mediana e desvio-padrão )..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "for entry in x_scaled:\n",
    "    if entry[57] < 1:\n",
    "        entry[57] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  4.48179282e-02,  1.25490189e-01, ...,\n",
       "         6.00720895e-03,  1.74873751e-02,  1.00000000e+00],\n",
       "       [ 4.62555066e-02,  1.96078438e-02,  9.80392173e-02, ...,\n",
       "         1.00120148e-02,  6.48358613e-02,  1.00000000e+00],\n",
       "       [ 1.32158585e-02,  0.00000000e+00,  1.39215678e-01, ...,\n",
       "         4.84581478e-02,  1.42550513e-01,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 6.60792962e-02,  0.00000000e+00,  5.88235334e-02, ...,\n",
       "         5.00600727e-04,  7.38636404e-03, -1.00000000e+00],\n",
       "       [ 2.11453736e-01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         4.00480581e-04,  4.86111175e-03, -1.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  1.27450973e-01, ...,\n",
       "         4.00480581e-04,  2.46212143e-03, -1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descrição de Online SVM via Online Gradient Descent\n",
    "* No instante t\n",
    "    * Selecionamos um vetor pt\n",
    "    * Recebemos uma nova instância do dataset (yt, zt), onde yt são os atributos e zt é a classe correspondente\n",
    "    * Vamos tomar uma Hinge Loss definida como max(0, 1 - zt * <pt, yt> )\n",
    "    * Utilizaremos o gradiente da função de perda do instante t para calcular o próximo p(t+1)\n",
    "    \n",
    "Vamos também utilizar sempre pt pertencente ao espaço euclideano de norma <= 1.\n",
    "\n",
    "A princípio o p0 pode ser definido arbitrariamente, mas como vimos descrito em alguns lugares da literatura a inicialização com o vetor nulo vamos adotar essa estratégia.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui algumas funções auxiliares úteis\n",
    "def hinge_loss(p_t, z_t, y_t):\n",
    "    return max(0, 1 - np.dot(p_t, y_t) * z_t )\n",
    "\n",
    "def project_into_euclidean_norm_1( p_t ):\n",
    "    norm = np.linalg.norm(p_t)\n",
    "    norm_p = p_t.copy()\n",
    "    if norm <= 1:\n",
    "        return norm_p\n",
    "    else:\n",
    "        norm_p = np.true_divide( norm_p, norm )\n",
    "        return norm_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_svm(data, eta_fnc):\n",
    "    current_p = np.zeros(57)\n",
    "    did_classify_correctly = np.zeros(len(data))\n",
    "    all_p = [ current_p ]\n",
    "    cumulative_loss = 0\n",
    "    # Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "    t = 1\n",
    "\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "        target_class = instance[57]\n",
    "        # vamos tomar a perda hinge relativo a current_p\n",
    "        current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "        dot_prod = np.dot( current_p, instance_features )\n",
    "        predicted_class =  -1\n",
    "        if dot_prod >= 0:\n",
    "            predicted_class =  1\n",
    "        did_classify_correctly[t-1] = (target_class == predicted_class)\n",
    "        \n",
    "        cumulative_loss += current_loss\n",
    "        # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "        # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "\n",
    "        # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "        if current_loss > 0:\n",
    "            gradient = instance_features * (-target_class) # derivada\n",
    "            current_eta = eta_fnc( t ) # Eta calculado em funcao de t, de acordo com a funcao recebida como parametro\n",
    "            \n",
    "            # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "            current_p -= current_eta * gradient\n",
    "            current_p = project_into_euclidean_norm_1( current_p )\n",
    "            \n",
    "        tmp = np.append(all_p, [current_p], axis = 0 ) # Adding current support vector to the list\n",
    "        all_p = tmp.copy()\n",
    "        \n",
    "        t += 1 # increment in one the number of processed instances\n",
    "    return {\"p\": all_p, \"did_classify_correctly\": did_classify_correctly}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta( t ):\n",
    "    return 1 / sqrt(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_scaled.copy()\n",
    "\n",
    "results = online_svm( test_data, eta )\n",
    "p = results[\"p\"]\n",
    "did_classify_correctly_not_shuffled = results[\"did_classify_correctly\"]\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle( test_data )\n",
    "\n",
    "results2 = online_svm( test_data, eta )\n",
    "p2 = results2[\"p\"]\n",
    "did_classify_correctly_shuffled = results2[\"did_classify_correctly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unshuffled_df = pd.DataFrame(did_classify_correctly_not_shuffled, columns=[\"is_correct\"])\n",
    "shuffled_df = pd.DataFrame(did_classify_correctly_shuffled, columns=[\"is_correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4601"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_unshuffled = unshuffled_df[unshuffled_df[\"is_correct\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_shuffled = shuffled_df[shuffled_df[\"is_correct\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_unshuffled = 1 - len(incorrect_unshuffled) / len(data)\n",
    "accuracy_shuffled = 1 - len(incorrect_shuffled) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9547924364268637\n",
      "0.7546185611823517\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_unshuffled)\n",
    "print(accuracy_shuffled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
