{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/spambase.data\", dtype='f', delimiter=',')\n",
    "\n",
    "print (data)\n",
    "\n",
    "max_value_in_data = [ 0 ] * 58\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 58):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que teremos que tratar os dados antes de rodar __Online SVM via Online Gradient Descent__\n",
    "\n",
    "Temos ao menos duas etapas:\n",
    "1 - Transformar os valores de classe ( variável target ) de 0, 1 para -1 ,1\n",
    "2 - Temos que fazer uma normalização dos parâmetros. Pois alguns dados são percentuais e estão no range [0, 100] e outros podem atingir valores maiores como por exemplo 15841. Não queremos que um atributo tenha uma importância maior que outro por ser representado em uma escala diferente. Vamos fazer uma normalização __hard__ que é mapear o maior valor de uma determinada feature para 1 e dividir os outros valores dessa feature específica pelo maior valor encontrado.\n",
    "\n",
    "Se a normalização hard se mostrar ruim na prática, podemos pensar em outras normalizações. ( Soft usando mediana e desvio-padrão )..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos alterar os valores da variável target de (0, 1) para (-1, 1)\n",
    "for lin in data:\n",
    "    if lin[57] == 0:\n",
    "        lin[57] = -1\n",
    "        \n",
    "# Agora vamos fazer a normalizacao hard\n",
    "normalized_data = data\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 57):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        \n",
    "for lin in normalized_data:\n",
    "    for x in range(0, 57):\n",
    "        lin[x] = (lin[x] / max_value_in_data[x] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descrição de Online SVM via Online Gradient Descent\n",
    "* No instante t\n",
    "    * Selecionamos um vetor pt\n",
    "    * Recebemos uma nova instância do dataset (yt, zt), onde yt são os atributos e zt é a classe correspondente\n",
    "    * Vamos tomar uma Hinge Loss definida como max(0, 1 - zt * <pt, yt> )\n",
    "    * Utilizaremos o gradiente da função de perda do instante t para calcular o próximo p(t+1)\n",
    "    \n",
    "Vamos também utilizar sempre pt pertencente ao espaço euclideano de norma <= 1.\n",
    "\n",
    "A princípio o p0 pode ser definido arbitrariamente, mas como vimos descrito em alguns lugares da literatura a inicialização com o vetor nulo vamos adotar essa estratégia.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui algumas funções auxiliares úteis\n",
    "dimension = 57\n",
    "\n",
    "def dot_product( a, b ):\n",
    "    ans = 0\n",
    "    for x in range(dimension):\n",
    "        ans += ( a[x] * b[x] )\n",
    "    return ans\n",
    "\n",
    "def hinge_loss( p_t, z_t, y_t):\n",
    "    return max(0, 1 - dot_product(p_t, y_t) * z_t )\n",
    "\n",
    "def euclidean_norm( p_t ):\n",
    "    ans = 0\n",
    "    for coord in p_t:\n",
    "        ans += (coord ** 2)\n",
    "    return sqrt( ans )\n",
    "\n",
    "def project_into_euclidean_norm_1( p_t ):\n",
    "    norm = euclidean_norm( p_t )\n",
    "    if norm <= 1:\n",
    "        return p_t\n",
    "    \n",
    "    norm_p = p_t\n",
    "    for coord in norm_p:\n",
    "        coord = ( coord / norm )\n",
    "    return norm_p\n",
    "\n",
    "def get_gradient( y_t, z_t ):\n",
    "    gradient = y_t\n",
    "    for x in gradient:\n",
    "        x *= ( - z_t )\n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perda na iteracao 100 = 2.233652\n",
      "perda na iteracao 200 = 1.971643\n",
      "perda na iteracao 300 = 2.754252\n",
      "perda na iteracao 400 = 2.506226\n",
      "perda na iteracao 500 = 3.998994\n",
      "perda na iteracao 600 = 3.742833\n",
      "perda na iteracao 700 = 4.484138\n",
      "perda na iteracao 800 = 4.962326\n",
      "perda na iteracao 900 = 6.403390\n",
      "perda na iteracao 1000 = 7.638121\n",
      "perda na iteracao 1100 = 7.696410\n",
      "perda na iteracao 1200 = 4.018514\n",
      "perda na iteracao 1300 = 4.582764\n",
      "perda na iteracao 1400 = 5.675093\n",
      "perda na iteracao 1500 = 6.675964\n",
      "perda na iteracao 1600 = 3.738295\n",
      "perda na iteracao 1700 = 5.376902\n",
      "perda na iteracao 1800 = 1.584158\n",
      "perda na iteracao 1900 = 0.956205\n",
      "perda na iteracao 2000 = 0.504747\n",
      "perda na iteracao 2100 = 0.000000\n",
      "perda na iteracao 2200 = 0.000000\n",
      "perda na iteracao 2300 = 0.000000\n",
      "perda na iteracao 2400 = 0.000000\n",
      "perda na iteracao 2500 = 0.000000\n",
      "perda na iteracao 2600 = 0.000000\n",
      "perda na iteracao 2700 = 0.000000\n",
      "perda na iteracao 2800 = 0.996728\n",
      "perda na iteracao 2900 = 0.000000\n",
      "perda na iteracao 3000 = 0.000000\n",
      "perda na iteracao 3100 = 0.220260\n",
      "perda na iteracao 3200 = 0.998982\n",
      "perda na iteracao 3300 = 0.000000\n",
      "perda na iteracao 3400 = 0.824030\n",
      "perda na iteracao 3500 = 0.000000\n",
      "perda na iteracao 3600 = 0.489273\n",
      "perda na iteracao 3700 = 0.000000\n",
      "perda na iteracao 3800 = 0.000000\n",
      "perda na iteracao 3900 = 0.997307\n",
      "perda na iteracao 4000 = 0.000000\n",
      "perda na iteracao 4100 = 0.000000\n",
      "perda na iteracao 4200 = 0.198570\n",
      "perda na iteracao 4300 = 0.000000\n",
      "perda na iteracao 4400 = 0.000000\n",
      "perda na iteracao 4500 = 0.000000\n",
      "perda na iteracao 4600 = 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vamos armazenar os pt obtidos ao longo do tempo, para poder analisar quão rápido o método alcança bons classificadores\n",
    "# E no final vamos testar os valores obtidos como classificadores para o dataset completo e analisar a acurácia\n",
    "all_p = []\n",
    "\n",
    "current_p = [0] * 57\n",
    "\n",
    "all_p.append( current_p )\n",
    "\n",
    "# Temos algumas estratégias para definir o eta utilizado para atualizar p_t\n",
    "# Vamos usar o eta proposto por Zinkevich que é da ordem de 1/sqrt(t)\n",
    "# Que vai diminuindo conforme a quantidade de instancias processadas aumenta\n",
    "\n",
    "# Se tivéssemos a segunda derivada da função de perda poderiamos usar \n",
    "# uma outra estratégia descoberta por Hazan em que eta_t = 1 / Ht, onde H é \n",
    "# um valor relacionado com a segunda derivada\n",
    "\n",
    "# Essas estratégias para eta são definidas aqui (http://www.mit.edu/~rakhlin/papers/adaptive.pdf)\n",
    "\n",
    "\n",
    "cumulative_loss = 0\n",
    "# Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "\n",
    "t = 1\n",
    "\n",
    "for instance in normalized_data:\n",
    "    instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "    target_class = instance[57]\n",
    "    # vamos tomar a perda hinge relativo a current_p\n",
    "    current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "    cumulative_loss += current_loss\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print('perda na iteracao %d = %f' % (t, current_loss) )\n",
    "    \n",
    "    # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "    # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "    \n",
    "    # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "    if current_loss != 0:\n",
    "        gradient = get_gradient( instance_features, target_class )\n",
    "        current_eta = 1 / sqrt(t)\n",
    "        # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "        for coord in range(57):\n",
    "            current_p[coord] = (current_p[coord] - current_eta * gradient[coord])\n",
    "        \n",
    "        current_p = project_into_euclidean_norm_1( current_p )\n",
    "            \n",
    "        \n",
    "    all_p.append( current_p ) # Adding current support vector to the list\n",
    "    t += 1 # increment in one the number of processed instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora __encapsular__ o que foi implementado acima em um método, para poder testar diversos parâmetros de __eta__. E também vamos fazer uma função que mede a acurácia de um determinado vetor __p__ para classificar os dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_svm(data, eta_fnc):\n",
    "    current_p = [0] * 57\n",
    "    all_p = [current_p]\n",
    "    cumulative_loss = 0\n",
    "    # Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "    t = 1\n",
    "\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "        target_class = instance[57]\n",
    "        # vamos tomar a perda hinge relativo a current_p\n",
    "        current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "        cumulative_loss += current_loss\n",
    "\n",
    "        # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "        # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "\n",
    "        # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "        if current_loss != 0:\n",
    "            gradient = get_gradient( instance_features, target_class )\n",
    "            current_eta = eta_fnc( t ) # Eta calculado em funcao de t, de acordo com a funcao recebida como parametro\n",
    "            \n",
    "            # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "            for coord in range(57):\n",
    "                current_p[coord] = (current_p[coord] - current_eta * gradient[coord])\n",
    "\n",
    "            current_p = project_into_euclidean_norm_1( current_p )\n",
    "\n",
    "\n",
    "        all_p.append( current_p ) # Adding current support vector to the list\n",
    "        t += 1 # increment in one the number of processed instances\n",
    "    return (cumulative_loss, all_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que encapsulamos o método em uma função que recebe como parâmetro uma função para o cálculo de eta, podemos começar a realizar alguns experimentos.\n",
    "\n",
    "Antes disso, vou criar uma função que recebe um determinado vetor p, usa ele para __classificar__ todo o dataset e retorna a acurácia desse vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n",
      "0.6059552271245382\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(data, classifier_vector):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57]\n",
    "        target_class = instance[57]\n",
    "        dot_prod = dot_product( instance_features, classifier_vector )\n",
    "        predicted_class = -1\n",
    "        if dot_prod >= 0:\n",
    "            predicted_class = 1\n",
    "        if target_class == predicted_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return ( correct / total )\n",
    "\n",
    "\n",
    "def eta( t ):\n",
    "    return 1 / sqrt(t)\n",
    "\n",
    "def eta_constante( t ):\n",
    "    return 0.5\n",
    "\n",
    "results = online_svm( normalized_data, eta_constante )\n",
    "\n",
    "\n",
    "melhor = 0\n",
    "k = 0\n",
    "for x in results[1]:\n",
    "    if k % 100 == 0:\n",
    "        #print(x)\n",
    "        val = check_accuracy( normalized_data, x)\n",
    "        if val > melhor:\n",
    "            melhor = val\n",
    "        print( val )\n",
    "    k += 1\n",
    "print(melhor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
