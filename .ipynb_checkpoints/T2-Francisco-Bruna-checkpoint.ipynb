{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.54,\n",
       " 14.28,\n",
       " 5.1,\n",
       " 42.81,\n",
       " 10.0,\n",
       " 5.88,\n",
       " 7.27,\n",
       " 11.11,\n",
       " 5.26,\n",
       " 18.18,\n",
       " 2.61,\n",
       " 9.67,\n",
       " 5.55,\n",
       " 10.0,\n",
       " 4.41,\n",
       " 20.0,\n",
       " 7.14,\n",
       " 9.09,\n",
       " 18.75,\n",
       " 18.18,\n",
       " 11.11,\n",
       " 17.1,\n",
       " 5.45,\n",
       " 12.5,\n",
       " 20.83,\n",
       " 16.66,\n",
       " 33.33,\n",
       " 9.09,\n",
       " 14.28,\n",
       " 5.88,\n",
       " 12.5,\n",
       " 4.76,\n",
       " 18.18,\n",
       " 4.76,\n",
       " 20.0,\n",
       " 7.69,\n",
       " 6.89,\n",
       " 8.33,\n",
       " 11.11,\n",
       " 4.76,\n",
       " 7.14,\n",
       " 14.28,\n",
       " 3.57,\n",
       " 20.0,\n",
       " 21.42,\n",
       " 22.05,\n",
       " 2.17,\n",
       " 10.0,\n",
       " 4.385,\n",
       " 9.752,\n",
       " 4.081,\n",
       " 32.478,\n",
       " 6.003,\n",
       " 19.829,\n",
       " 1102.5,\n",
       " 9989.0,\n",
       " 15841.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/spambase.data\", dtype='f', delimiter=',')\n",
    "\n",
    "print (data)\n",
    "\n",
    "max_value_in_data = [ 0 ] * 58\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 58):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que teremos que tratar os dados antes de rodar __Online SVM via Online Gradient Descent__\n",
    "\n",
    "Temos ao menos duas etapas:\n",
    "1 - Transformar os valores de classe ( variável target ) de 0, 1 para -1 ,1\n",
    "2 - Temos que fazer uma normalização dos parâmetros. Pois alguns dados são percentuais e estão no range [0, 100] e outros podem atingir valores maiores como por exemplo 15841. Não queremos que um atributo tenha uma importância maior que outro por ser representado em uma escala diferente. Vamos fazer uma normalização __hard__ que é mapear o maior valor de uma determinada feature para 1 e dividir os outros valores dessa feature específica pelo maior valor encontrado.\n",
    "\n",
    "Se a normalização hard se mostrar ruim na prática, podemos pensar em outras normalizações. ( Soft usando mediana e desvio-padrão )..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a54d00d47428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_value_in_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnormalized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# vamos alterar os valores da variável target de (0, 1) para (-1, 1)\n",
    "for lin in data:\n",
    "    if lin[57] == 0:\n",
    "        lin[57] = -1\n",
    "        \n",
    "# Agora vamos fazer a normalizacao hard\n",
    "normalized_data = data\n",
    "\n",
    "for lin in data:\n",
    "    for x in range(0, 57):\n",
    "        max_value_in_data[x] = max( max_value_in_data[x], lin[x] )\n",
    "        \n",
    "for lin in normalized_data:\n",
    "    for x in range(0, 57):\n",
    "        lin[x] = (lin[x] / max_value_in_data[x] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descrição de Online SVM via Online Gradient Descent\n",
    "* No instante t\n",
    "    * Selecionamos um vetor pt\n",
    "    * Recebemos uma nova instância do dataset (yt, zt), onde yt são os atributos e zt é a classe correspondente\n",
    "    * Vamos tomar uma Hinge Loss definida como max(0, 1 - zt * <pt, yt> )\n",
    "    * Utilizaremos o gradiente da função de perda do instante t para calcular o próximo p(t+1)\n",
    "    \n",
    "Vamos também utilizar sempre pt pertencente ao espaço euclideano de norma <= 1.\n",
    "\n",
    "A princípio o p0 pode ser definido arbitrariamente, mas como vimos descrito em alguns lugares da literatura a inicialização com o vetor nulo vamos adotar essa estratégia.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui algumas funções auxiliares úteis\n",
    "dimension = 57\n",
    "\n",
    "def dot_product( a, b ):\n",
    "    ans = 0\n",
    "    for x in range(dimension):\n",
    "        ans += ( a[x] * b[x] )\n",
    "    return ans\n",
    "\n",
    "def hinge_loss( p_t, z_t, y_t):\n",
    "    return max(0, 1 - dot_product(p_t, y_t) * z_t )\n",
    "\n",
    "def euclidean_norm( p_t ):\n",
    "    ans = 0\n",
    "    for coord in p_t:\n",
    "        ans += (coord ** 2)\n",
    "    return sqrt( ans )\n",
    "\n",
    "def project_into_euclidean_norm_1( p_t ):\n",
    "    norm = euclidean_norm( p_t )\n",
    "    if norm <= 1:\n",
    "        return p_t\n",
    "    \n",
    "    norm_p = p_t\n",
    "    for coord in norm_p:\n",
    "        coord = ( coord / norm )\n",
    "    return norm_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Vamos armazenar os pt obtidos ao longo do tempo, para poder analisar quão rápido o método alcança bons classificadores\n",
    "# E no final vamos testar os valores obtidos como classificadores para o dataset completo e analisar a acurácia\n",
    "all_p = []\n",
    "\n",
    "\n",
    "p = [0] * 57\n",
    "all_p.append(p)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
