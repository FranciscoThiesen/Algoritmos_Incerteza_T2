{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 6.400e-01 6.400e-01 ... 6.100e+01 2.780e+02 1.000e+00]\n",
      " [2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/spambase.data\", dtype='f', delimiter=',')\n",
    "\n",
    "print (data)\n",
    "print (data.dtype)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso mostra que teremos que tratar os dados antes de rodar __Online SVM via Online Gradient Descent__\n",
    "\n",
    "Temos ao menos duas etapas:\n",
    "1 - Transformar os valores de classe ( variável target ) de 0, 1 para -1 ,1\n",
    "2 - Temos que fazer uma normalização dos parâmetros. Pois alguns dados são percentuais e estão no range [0, 100] e outros podem atingir valores maiores como por exemplo 15841. Não queremos que um atributo tenha uma importância maior que outro por ser representado em uma escala diferente. Vamos fazer uma normalização __hard__ que é mapear o maior valor de uma determinada feature para 1 e dividir os outros valores dessa feature específica pelo maior valor encontrado.\n",
    "\n",
    "Se a normalização hard se mostrar ruim na prática, podemos pensar em outras normalizações. ( Soft usando mediana e desvio-padrão )..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_value_in_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a4fe2a68c9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m57\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmax_value_in_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmax_value_in_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmin_max_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_value_in_data' is not defined"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "# vamos alterar os valores da variável target de (0, 1) para (-1, 1)\n",
    "for entry in x_scaled:\n",
    "    if entry[57] < 1:\n",
    "        entry[57] = -1.0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breve descrição de Online SVM via Online Gradient Descent\n",
    "* No instante t\n",
    "    * Selecionamos um vetor pt\n",
    "    * Recebemos uma nova instância do dataset (yt, zt), onde yt são os atributos e zt é a classe correspondente\n",
    "    * Vamos tomar uma Hinge Loss definida como max(0, 1 - zt * <pt, yt> )\n",
    "    * Utilizaremos o gradiente da função de perda do instante t para calcular o próximo p(t+1)\n",
    "    \n",
    "Vamos também utilizar sempre pt pertencente ao espaço euclideano de norma <= 1.\n",
    "\n",
    "A princípio o p0 pode ser definido arbitrariamente, mas como vimos descrito em alguns lugares da literatura a inicialização com o vetor nulo vamos adotar essa estratégia.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui algumas funções auxiliares úteis\n",
    "def hinge_loss( p_t, z_t, y_t):\n",
    "    return max(0, 1 - np.dot(p_t, y_t) * z_t )\n",
    "\n",
    "def project_into_euclidean_norm_1( p_t ):\n",
    "    norm = np.linalg.norm(p_t)\n",
    "    norm_p = p_t.copy()\n",
    "    if norm <= 1:\n",
    "        return norm_p\n",
    "    else:\n",
    "        norm_p = np.true_divide( norm_p, norm )\n",
    "        return norm_p\n",
    "\n",
    "def get_gradient( y_t, z_t ):\n",
    "    gradient = y_t * (-z_t)\n",
    "    return gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora __encapsular__ o que foi implementado acima em um método, para poder testar diversos parâmetros de __eta__. E também vamos fazer uma função que mede a acurácia de um determinado vetor __p__ para classificar os dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_svm(data, eta_fnc):\n",
    "    current_p = np.zeros(57)\n",
    "    all_p = [ current_p ]\n",
    "    cumulative_loss = 0\n",
    "    # Agora vamos processar o dataset uma instancia de cada vez, num setup online\n",
    "    t = 1\n",
    "\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57] # slices are semi-open intervals (:, [0, 57)\n",
    "        target_class = instance[57]\n",
    "        # vamos tomar a perda hinge relativo a current_p\n",
    "        current_loss = hinge_loss( current_p, target_class, instance_features )\n",
    "        cumulative_loss += current_loss\n",
    "        # Agora vamos usar o gradiente / subgradiente para fazer a atualizacao de p_(t + 1)\n",
    "        # Se hinge_loss = 0, posso usar o subgradiente definido pela reta y = 0, e não mexer em p_t\n",
    "\n",
    "        # O caso de fato interessante é quando a perda é positiva e queremos seguir no sentido contrário do subgradiente\n",
    "        if current_loss > 0:\n",
    "            gradient = get_gradient( instance_features, target_class )\n",
    "            current_eta = eta_fnc( t ) # Eta calculado em funcao de t, de acordo com a funcao recebida como parametro\n",
    "            \n",
    "            # p_t = p_(t - 1) - eta_t * ( gradiente da função de perda )\n",
    "            cpy = current_p.copy()\n",
    "            \n",
    "            for coord in range(57):\n",
    "                cpy[coord] = (current_p[coord] - current_eta * gradient[coord])\n",
    "\n",
    "            cpy = project_into_euclidean_norm_1( cpy )\n",
    "            current_p = cpy.copy()\n",
    "            \n",
    "        tmp = np.append(all_p, [cpy], axis = 0 ) # Adding current support vector to the list\n",
    "        all_p = tmp.copy()\n",
    "        \n",
    "        t += 1 # increment in one the number of processed instances\n",
    "    return all_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que encapsulamos o método em uma função que recebe como parâmetro uma função para o cálculo de eta, podemos começar a realizar alguns experimentos.\n",
    "\n",
    "Antes disso, vou criar uma função que recebe um determinado vetor p, usa ele para __classificar__ todo o dataset e retorna a acurácia desse vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(data, classifier_vector):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for instance in data:\n",
    "        instance_features = instance[0:57].copy()\n",
    "        target_class = instance[57].copy()\n",
    "        dot_prod = np.dot( instance_features, classifier_vector )\n",
    "        predicted_class = -1\n",
    "        if dot_prod >= 0:\n",
    "            predicted_class = 1\n",
    "        if target_class == predicted_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return ( correct / total )\n",
    "\n",
    "\n",
    "def eta( t ):\n",
    "    return 1 / sqrt(t)\n",
    "\n",
    "def eta_constante( t ):\n",
    "    return 0.05\n",
    "\n",
    "test_data = x_scaled.copy()\n",
    "\n",
    "results2 = online_svm( test_data, eta )\n",
    "start_acc = check_accuracy( test_data, results2[0])\n",
    "end_acc = check_accuracy( test_data, results2[-1])\n",
    "print( start_acc, end_acc )\n",
    "\n",
    "np.random.seed(13)\n",
    "np.random.shuffle( test_data )\n",
    "\n",
    "results = online_svm( test_data, eta )\n",
    "start_acc2 = check_accuracy( test_data, results[0])\n",
    "end_acc2 = check_accuracy( test_data, results[-1])\n",
    "print( start_acc2, end_acc2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidimos realizar um shuffle nos dados antes de rodar, porque o dataset está organizado de uma forma que prejudica bastante a obtenção de um bom SVM no quesito acurácia.\n",
    "\n",
    "O dataset segue uma ordem 'ruim', no sentido que primeiro são fornecidos sequencialmente 1800 exemplos da classe __spam__ e depois vem > 2000 exemplos da classe __not spam__. Como nosso parâmetro __eta__ é definido como __1 / sqrt( amostras processadas )__, nosso algoritmo é bem pouco sensível as mudanças quando inserimos os exemplos da classe __not spam__.\n",
    "\n",
    "É interessante notar que o \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
